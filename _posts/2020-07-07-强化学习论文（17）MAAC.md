---


title:      "强化学习论文（17）MAAC"
subtitle:   "Actor-Attention-Critic for Multi-Agent Reinforcement Learning"
date:       2020-07-07 10:00:00

mathjax: true
tags:
    - 强化学习
    - 强化学习论文
---



**标签：** MAAC; actor-critic; off-policy; model-free; communication; continuous communication channel; discrete action space; continuous state space; mixed task; cooperative task; competitive task; centralized training with decentralized execution; multi-agent;



[论文链接](https://arxiv.org/abs/1810.02912)



## 创新点及贡献

1、基于 actor-critic 的 multi-agent 算法，其中每个智能体都有自己独立的 actor 和 critic，通过引入 Attention、SAC、Counterfactual Baseline 实现 MAAC 算法。



## 研究痛点

1、文章重点研究的是 Attention 机制与多智能体强化学习的结合，不同时考虑所有智能体的信息而只考虑重要的智能体的信息。



## 算法流程

本文采用 actor-critic 算法，其中 critic 的框架如下

<img width="100%" src="/images/in-post/2020-07-07-强化学习论文（17）MAAC.assets/image-20200707213124749.png"/>



### 主要思路

1、基于 actor-critic 的 multi-agent 算法，其中每个智能体都有自己独立的 actor 和 critic，并且引入了 SAC、Counterfactual Baseline 的方法，主要在 critic 层实现了 Attention 机制，下面着重介绍。

- critic 网络输出的是每个动作的 Q 值
- actor 网络输出的是每个动作的分布



2、每个智能体独立的 critic 的输入由全部智能体的观察和动作组成，在函数中又分为两部分，当前智能体的观察动作的嵌入向量及对其他智能体的进行 attention 计算得到的嵌入向量。

<img width="80%" src="/images/in-post/2020-07-07-强化学习论文（17）MAAC.assets/image-20200707214313121.png"/>



3、对其他智能体进行 attention 计算的公式就是正常的 attention 计算

- 注意 attention 中的参数矩阵 $W_q,W_k,V$ 都是共享的
- 此外还采用了 multiple attention heads 的方法，并简单的 concatenate 所有的 heads 为一个向量
- 文章提到如果想加入全局状态信息可以增加多一个 $e$，但是文章没有做这方面的实验，因为他们认为他们的方法在局部观察上的表现已经足够好了，全局状态信息不是很必要。

<img width="80%" src="/images/in-post/2020-07-07-强化学习论文（17）MAAC.assets/image-20200707215018898.png"/>



### 损失函数

1、critic 损失函数

<img width="80%" src="/images/in-post/2020-07-07-强化学习论文（17）MAAC.assets/image-20200707220120491.png"/>



2、actor 损失函数

- 与 MADDPG 不同，此处的 action 是从当前的策略函数进行采样的，而不是从 replay buffer 中进行采样，因为从 replay buffer 中进行采样会导致 overgeneralization，从而使得智能体之间无法根据当前策略进行有效协调。

<img width="80%" src="/images/in-post/2020-07-07-强化学习论文（17）MAAC.assets/image-20200707220143890.png"/>

- advantage function 如下

<img width="80%" src="/images/in-post/2020-07-07-强化学习论文（17）MAAC.assets/image-20200707220218267.png"/>

- Counterfactual Baseline 如下

<img width="80%" src="/images/in-post/2020-07-07-强化学习论文（17）MAAC.assets/image-20200707220309116.png"/>



### 算法伪代码

<img width="80%" src="/images/in-post/2020-07-07-强化学习论文（17）MAAC.assets/image-20200707221255394.png"/>

<img width="80%" src="/images/in-post/2020-07-07-强化学习论文（17）MAAC.assets/image-20200707221301893.png"/>





## 实验

1、在三个环境下进行实验

- Cooperative Treasure Collection
- Rover-Tower
- Cooperative Navigation

<img width="60%" src="/images/in-post/2020-07-07-强化学习论文（17）MAAC.assets/image-20200707213651558.png"/>



## 其他补充

1、本文从 critic 入手设计 attention 架构的思路还是挺直接的，但实验部分感觉对 attention 的分析比较少。



2、论文在实验部分证明了其 Scalability 能力较强。



3、论文提出其方法中每个智能体的动作空间可以是不一样的，奖励可以是不一样的，而且可以用于协作或竞争的环境中，但是在实验部分没有对这方面进行实验。



## 参考资料及致谢

所有参考资料在《强化学习思考（1）前言》中已列出，再次向强化学习的大佬前辈们表达衷心的感谢！

